{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это оригинальный код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "morph = MorphAnalyzer()\n",
    "stops = set(stopwords.words('russian'))\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "​\n",
    "PATH_TO_DATA = 'D:\\pycharm\\KeyWordExtraction\\\\venv\\data'\n",
    "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA)]\n",
    "data = pd.concat([pd.read_json(file, lines=True, encoding='utf-8') for file in files][:1], axis=0, ignore_index=True)\n",
    "​\n",
    "​\n",
    "def evaluate(true_kws, predicted_kws):\n",
    "    assert len(true_kws) == len(predicted_kws)\n",
    "​\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    jaccards = []\n",
    "​\n",
    "    for i in range(len(true_kws)):\n",
    "        true_kw = set(true_kws[i])\n",
    "        predicted_kw = set(predicted_kws[i])\n",
    "​\n",
    "        tp = len(true_kw & predicted_kw)\n",
    "        union = len(true_kw | predicted_kw)\n",
    "        fp = len(predicted_kw - true_kw)\n",
    "        fn = len(true_kw - predicted_kw)\n",
    "​\n",
    "        if (tp + fp) == 0:\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = tp / (tp + fp)\n",
    "​\n",
    "        if (tp + fn) == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp / (tp + fn)\n",
    "        if (prec + rec) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2 * (prec * rec)) / (prec + rec)\n",
    "​\n",
    "        jac = tp / union\n",
    "​\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jac)\n",
    "    print('Precision - ', round(np.mean(precisions), 2))\n",
    "    print('Recall - ', round(np.mean(recalls), 2))\n",
    "    print('F1 - ', round(np.mean(f1s), 2))\n",
    "    print('Jaccard - ', round(np.mean(jaccards), 2))\n",
    "​\n",
    "​\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "​\n",
    "punct = punctuation + '«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "​\n",
    "def normalize(text):\n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.normal_form for word in words if word.tag.POS == 'NOUN']\n",
    "​\n",
    "    return words\n",
    "​\n",
    "data['content_norm'] = data['content'].apply(normalize)\n",
    "data['title_norm'] = data['title'].apply(normalize)\n",
    "data['content_norm_str'] = data['content_norm'].apply(' '.join)\n",
    "​\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
    "tfidf.fit(data['content_norm_str'])\n",
    "​\n",
    "​\n",
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
    "texts_vectors = tfidf.transform(data['content_norm_str'])\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-11:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый мотод: больше нормализации. В результате есть имя \"том\", которое вероятно не будет ключевым словом. Пожтому я сделал список 5000 имен на русском языке и удалил из результаты имена есть они в сптслк имен. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt','r',encoding='utf-8') as names:\n",
    "    names = names.read()\n",
    "    names = names.lower()\n",
    "    names = names.split('\\n')\n",
    "​\n",
    "for entry in keywords:\n",
    "    for word in entry:\n",
    "        if entry in names:\n",
    "            entry.remove(word)\n",
    "​\n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Precision -  0.13\n",
    "Recall -  0.24\n",
    "F1 -  0.16\n",
    "Jaccard -  0.09\n",
    "​\n",
    "К сожалению, улучшения нет."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Второй метод: В результате есть одновремено \"нью\", \"иорк\" и \"нью иорк\". Чтобы удалить это повторение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in keywords:\n",
    "    for word_1 in entry:\n",
    "        for word_2 in entry:\n",
    "            if word_1 + ' ' + word_2 in entry or word_2 + ' ' + word_1 in entry:\n",
    "​\n",
    "                try:\n",
    "                    entry.remove(word_1)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    entry.remove(word_2)\n",
    "                except:\n",
    "                    pass\n",
    "​\n",
    "​\n",
    "​\n",
    "            if word_1 != word_2:\n",
    "                if word_1 != word_2:\n",
    "                    try:\n",
    "                        entry.remove(word_1)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Precision -  0.11\n",
    "Recall -  0.1\n",
    "F1 -  0.1\n",
    "Jaccard -  0.06\n",
    "Даже хуже. Это значит что есть ситуации, в которых одно слово ключевое, а фраза нет."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Третий метод: 10 слов в результате кажется слишком много. Снижаю в 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-6:-1]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Precision -  0.19\n",
    "Recall -  0.18\n",
    "F1 -  0.17\n",
    "Jaccard -  0.11\n",
    "Улучшение есть."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Четвертый метод: я предполагаю, что слова, которые были кличеывми в других текстах, возможнее будят ключевыми. Если слово никогда не было ключевым, надо удалить его из результата."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Пусть results.txt будет список все унормализованные ключевые слова всех текста в всех json файлах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.txt','r',encoding='utf-8') as results:\n",
    "    results = results.read()\n",
    "    results = results.split('\\n')\n",
    "    for entry in keywords:\n",
    "        for word in entry:\n",
    "            if word not in results:\n",
    "                entry.remove(word)\n",
    "                \n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Precision -  0.18134\n",
    "Recall -  0.22179\n",
    "F1 -  0.1888\n",
    "Jaccard -  0.11342\n",
    "(С keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-11:-1]])\n",
    "Ура, очевидное улучшение!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Если с:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-7:-1]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Precision -  0.2379\n",
    "Recall -  0.18525\n",
    "F1 -  0.19652\n",
    "Jaccard -  0.12198\n",
    "Даже лучше."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
